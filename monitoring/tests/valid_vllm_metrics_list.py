VALID_VLLM_METRIC_LIST = [
    "vllm:cache_config_info",
    "vllm:e2e_request_latency_seconds_bucket",
    "vllm:e2e_request_latency_seconds_count",
    "vllm:e2e_request_latency_seconds_created",
    "vllm:e2e_request_latency_seconds_sum",
    "vllm:engine_sleep_state",
    "vllm:external_prefix_cache_hits_created",
    "vllm:external_prefix_cache_hits_total",
    "vllm:external_prefix_cache_queries_created",
    "vllm:external_prefix_cache_queries_total",
    "vllm:generation_tokens_created",
    "vllm:generation_tokens_total",
    "vllm:inter_token_latency_seconds_bucket",
    "vllm:inter_token_latency_seconds_count",
    "vllm:inter_token_latency_seconds_created",
    "vllm:inter_token_latency_seconds_sum",
    "vllm:iteration_tokens_total_bucket",
    "vllm:iteration_tokens_total_count",
    "vllm:iteration_tokens_total_created",
    "vllm:iteration_tokens_total_sum",
    "vllm:kv_cache_usage_perc",
    "vllm:mm_cache_hits_created",
    "vllm:mm_cache_hits_total",
    "vllm:mm_cache_queries_created",
    "vllm:mm_cache_queries_total",
    "vllm:num_preemptions_created",
    "vllm:num_preemptions_total",
    "vllm:num_requests_running",
    "vllm:num_requests_waiting",
    "vllm:prefix_cache_hits_created",
    "vllm:prefix_cache_hits_total",
    "vllm:prefix_cache_queries_created",
    "vllm:prefix_cache_queries_total",
    "vllm:prompt_tokens_created",
    "vllm:prompt_tokens_total",
    "vllm:request_decode_time_seconds_bucket",
    "vllm:request_decode_time_seconds_count",
    "vllm:request_decode_time_seconds_created",
    "vllm:request_decode_time_seconds_sum",
    "vllm:request_generation_tokens_bucket",
    "vllm:request_generation_tokens_count",
    "vllm:request_generation_tokens_created",
    "vllm:request_generation_tokens_sum",
    "vllm:request_inference_time_seconds_bucket",
    "vllm:request_inference_time_seconds_count",
    "vllm:request_inference_time_seconds_created",
    "vllm:request_inference_time_seconds_sum",
    "vllm:request_max_num_generation_tokens_bucket",
    "vllm:request_max_num_generation_tokens_count",
    "vllm:request_max_num_generation_tokens_created",
    "vllm:request_max_num_generation_tokens_sum",
    "vllm:request_params_max_tokens_bucket",
    "vllm:request_params_max_tokens_count",
    "vllm:request_params_max_tokens_created",
    "vllm:request_params_max_tokens_sum",
    "vllm:request_params_n_bucket",
    "vllm:request_params_n_count",
    "vllm:request_params_n_created",
    "vllm:request_params_n_sum",
    "vllm:request_prefill_time_seconds_bucket",
    "vllm:request_prefill_time_seconds_count",
    "vllm:request_prefill_time_seconds_created",
    "vllm:request_prefill_time_seconds_sum",
    "vllm:request_prompt_tokens_bucket",
    "vllm:request_prompt_tokens_count",
    "vllm:request_prompt_tokens_created",
    "vllm:request_prompt_tokens_sum",
    "vllm:request_queue_time_seconds_bucket",
    "vllm:request_queue_time_seconds_count",
    "vllm:request_queue_time_seconds_created",
    "vllm:request_queue_time_seconds_sum",
    "vllm:request_success_created",
    "vllm:request_success_total",
    "vllm:request_time_per_output_token_seconds_bucket",
    "vllm:request_time_per_output_token_seconds_count",
    "vllm:request_time_per_output_token_seconds_created",
    "vllm:request_time_per_output_token_seconds_sum",
    "vllm:time_per_output_token_seconds_bucket",
    "vllm:time_per_output_token_seconds_count",
    "vllm:time_per_output_token_seconds_created",
    "vllm:time_per_output_token_seconds_sum",
    "vllm:time_to_first_token_seconds_bucket",
    "vllm:time_to_first_token_seconds_count",
    "vllm:time_to_first_token_seconds_created",
    "vllm:time_to_first_token_seconds_sum"
]
