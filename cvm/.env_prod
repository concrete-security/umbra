# Model configuration (single source)
MODEL_ID=openai/gpt-oss-120b
MODEL_REVISION=main
MODEL_DIRNAME=gpt-oss-120b
DOWNLOAD_MODEL_AT_BUILD=0

# Primary vLLM runtime knobs
VLLM_REASONING_PARSER=openai_gptoss
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_MODEL_LEN=10240
VLLM_MAX_NUM_SEQS=1
VLLM_MAX_NUM_BATCHED_TOKENS=8192
VLLM_ASYNC_SCHEDULING=1
VLLM_NO_ENABLE_PREFIX_CACHING=1
VLLM_CUDA_GRAPH_SIZES=2048

# Storage configuration (default Linux)
HOST_MODEL_STORAGE_DIR=/mnt/disk1/tee_models
CONTAINER_MODEL_STORAGE_DIR=/mnt/disk1/tee_models
HOST_HF_CACHE=/mnt/disk1/huggingface

# Runtime / GPU configuration
VLLM_TARGET_DEVICE=cuda
VLLM_USE_V1=1
TP_SIZE=${VLLM_TENSOR_PARALLEL_SIZE}
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
USE_GPU=1

PYTHONUNBUFFERED=1

# Flash Attention (optional)
INSTALL_FLASH_ATTN=1
API_KEY=your-secret-api-key-here

# Attestation Configuration
VERIFY_DIR=/app/verify
QUOTE_PATH=/home/ubuntu/secure-llm/vllm_proxy_server/attest/quote.pb
ATTEST_URL=http://localhost:8080/attest
EXPECTED_RTMR3=

# Machine Configuration
MACHINE=test-machine
IS_CVM=false
URL=http://localhost:8000
VLLM_CONTAINER=vllm_container
####### URL and Endpoints Configuration ########

# HTTPS
DOMAIN_NAME=concrete.security
PORT_PROVIDER=8000
PORT_PROXY_RAG=7000

# Depends on the mode
IS_CVM=True
MACHINE=H100
